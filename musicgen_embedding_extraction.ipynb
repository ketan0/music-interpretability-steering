{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ketanagrawal/music-sae/.venv/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/Users/ketanagrawal/music-sae/.venv/lib/python3.12/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 2048])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "pad_token_id = model.generation_config.pad_token_id\n",
    "decoder_input_ids = (\n",
    "    torch.ones((inputs.input_ids.shape[0] * model.decoder.num_codebooks, 1), dtype=torch.long)\n",
    "    * pad_token_id\n",
    ")\n",
    "\n",
    "logits = model(**inputs, decoder_input_ids=decoder_input_ids).logits\n",
    "logits.shape  # (bsz * num_codebooks, tgt_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "MUSICGEN_NUM_DECODER_HIDDEN_LAYERS = 25\n",
    "MUSICGEN_HIDDEN_SIZE = 1024\n",
    "\n",
    "def get_decoder_hidden_states(audio_batch, text_batch, sampling_rate, mock=False, device='cpu'):\n",
    "    if mock:\n",
    "        return torch.randn(len(audio_batch), MUSICGEN_NUM_DECODER_HIDDEN_LAYERS, MUSICGEN_HIDDEN_SIZE)\n",
    "    inputs = processor(\n",
    "        text=text_batch,\n",
    "        audio=audio_batch,\n",
    "        sampling_rate=sampling_rate,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    pad_token_id = model.generation_config.pad_token_id\n",
    "    decoder_input_ids = (\n",
    "        torch.ones((inputs.input_ids.shape[0] * model.decoder.num_codebooks, 1), dtype=torch.long)\n",
    "        * pad_token_id\n",
    "    ).to(device)\n",
    "    # put inputs on device\n",
    "    inputs = inputs.to(device)\n",
    "    output = model(**inputs, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
    "\n",
    "    return torch.stack(output.decoder_hidden_states).transpose(0, 1).squeeze(2) # (num_layers, batch_size, 1, hidden_size) -> (batch_size, num_layers, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset to 696 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 10it [00:06,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 320 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 20it [00:13,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 640 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 22it [00:14,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import h5py\n",
    "import torchaudio\n",
    "import torch\n",
    "from pytube import YouTube\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    # Load a local music file using torchaudio\n",
    "    waveform, sampling_rate = torchaudio.load(file_path)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    return waveform.squeeze().numpy(), sampling_rate\n",
    "\n",
    "def process_and_save_embeddings(output_file, batch_size=32, device='cpu'):\n",
    "    model.to(device)\n",
    "    dataset = load_dataset(\"google/MusicCaps\", split=\"train\")\n",
    "    # now, filter the dataset for only records where ./music_data/{ytid}.mp3 exists\n",
    "    dataset = dataset.filter(lambda x: os.path.exists(f\"./music_data/{x['ytid']}.wav\"))\n",
    "    print(f\"Filtered dataset to {len(dataset)} samples\")\n",
    "    \n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        dset = None  # We'll create this once we know the embedding shape\n",
    "        \n",
    "        for i, batch in tqdm(enumerate(dataset.iter(batch_size)), desc=\"Processing batches\"):\n",
    "            text_batch = batch['caption']\n",
    "            audio_batch = []\n",
    "            for ytid in batch['ytid']:\n",
    "                # load the audio file from ./music_data/{ytid}.wav\n",
    "                waveform, sampling_rate = load_and_preprocess_audio(f\"./music_data/{ytid}.wav\")\n",
    "                audio_batch.append(waveform)\n",
    "            \n",
    "            hidden_states = get_decoder_hidden_states(audio_batch, text_batch, sampling_rate, device=device, mock=False)\n",
    "            \n",
    "            if dset is None:\n",
    "                # Create the dataset once we know the shape\n",
    "                dset = f.create_dataset('embeddings', \n",
    "                                        shape=(0,) + hidden_states.shape[1:],\n",
    "                                        maxshape=(None,) + hidden_states.shape[1:],\n",
    "                                        chunks=True, compression='gzip')\n",
    "            \n",
    "            # Resize the dataset and add new data\n",
    "            dset.resize(dset.shape[0] + hidden_states.shape[0], axis=0)\n",
    "            dset[-hidden_states.shape[0]:] = hidden_states.cpu().numpy()\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"Processed {(i+1)*batch_size} samples\")\n",
    "\n",
    "# Usage\n",
    "process_and_save_embeddings('musiccaps_embeddings.h5', batch_size=32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696, 25, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "# load the embeddings\n",
    "with h5py.File('musiccaps_embeddings.h5', 'r') as f:\n",
    "    embeddings = f['embeddings'][:]\n",
    "    print(embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (music-sae)",
   "language": "python",
   "name": "music-sae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
